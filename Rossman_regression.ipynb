{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# This should be the final version of the Rossman script\n",
    "The model is a ensemble of xgboosts with the same data.<br>\n",
    "The data used includes:\n",
    "    1. Simple aggregates for each store and initial data. Mostly without any preprocessing\n",
    "    2. Weather, dates of football games\n",
    "    3. Date derived features, so that the object will have features on how long is is till next promotion, till next holiday, etc.\n",
    "    \n",
    "Data cleaning includes deletion of days near holidays, as they add a lot of noise and are not relevant to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "store = pd.read_csv('store.csv')\n",
    "tr = pd.read_pickle('train_holidays_corrected')\n",
    "te = pd.read_pickle('test_holidays_corrected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function\n",
    "Left comments in Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_train_data(train, test, store, scaler = None):\n",
    "    \"\"\"\n",
    "    Препроцессинг данных\n",
    "    Inputs: \n",
    "        train, test - пара датафреймов, уже разбитые на train и test. Так сделано, потому что на трейне должны \n",
    "        считаться агрегаты и не должно быть утечки информации\n",
    "    \n",
    "    Outputs: \n",
    "        train, test - перезаписанные датафреймы\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # Препроцессинг даты\n",
    "    def preprocess_date(x):\n",
    "        x['Date'] = pd.to_datetime(x['Date'], format = '%Y-%m-%d')\n",
    "        x['day'] = x['Date'].dt.day\n",
    "        x['year'] = x['Date'].dt.year\n",
    "        x['month'] = x['Date'].dt.month\n",
    "        x['dayofyear'] = x['Date'].dt.dayofyear\n",
    "        x['weekofyear'] = x['Date'].dt.weekofyear\n",
    "        return x\n",
    "    \n",
    "    train = preprocess_date(train)\n",
    "    test = preprocess_date(test)\n",
    "    \n",
    "    train = train[(train.till_holiday>1) & (train.from_holiday>1)]\n",
    "    \n",
    "    # Расчет средних характеристик (возможно только на трейне, \n",
    "    # в других случаях буду цеплять к тесту через merge по идентификатору магазина)\n",
    "    def preprocess_fixed_effects(x, l):\n",
    "        for i in l:\n",
    "            x = pd.merge(x, i, on = 'Store')\n",
    "        return x\n",
    "\n",
    "    def kurt(x):\n",
    "        return pd.Series({'ku':x.Sales.kurtosis(axis = 0)})\n",
    "    \n",
    "    def sk(x):\n",
    "        return pd.Series({'sk':x.Sales.skew()})\n",
    "    \n",
    "    def frac_mean(x):\n",
    "        return np.mean(x['Sales'] / x['Customers'])\n",
    "    \n",
    "    def frac_median(x):\n",
    "        return np.median(x['Sales'] / x['Customers'])\n",
    "    \n",
    "    def replace_promo(x):\n",
    "        x.loc[:, 'PromoInterval_Feb,May,Aug,Nov'] = x.loc[:, ['PromoInterval_Feb,May,Aug,Nov', 'month']].prod(axis = 1) \n",
    "        x.loc[:, 'PromoInterval_Jan,Apr,Jul,Oct'] = x.loc[:, ['PromoInterval_Jan,Apr,Jul,Oct', 'month']].prod(axis = 1) \n",
    "        x.loc[:, 'PromoInterval_Mar,Jun,Sept,Dec'] = x.loc[:, ['PromoInterval_Mar,Jun,Sept,Dec', 'month']].prod(axis = 1) \n",
    "        x['PromoInterval_Feb,May,Aug,Nov'].replace({1: 2,\n",
    "                                                     2: 1,\n",
    "                                                     3: 2,\n",
    "                                                     4: 2,\n",
    "                                                     5: 1,\n",
    "                                                     6: 2,\n",
    "                                                     7: 2,\n",
    "                                                     8: 1,\n",
    "                                                     9: 2,\n",
    "                                                     10: 2,\n",
    "                                                     11: 1,\n",
    "                                                     12: 2}, inplace = True)\n",
    "        x['PromoInterval_Jan,Apr,Jul,Oct'].replace({1: 1,\n",
    "                                                     2: 2,\n",
    "                                                     3: 2,\n",
    "                                                     4: 1,\n",
    "                                                     5: 2,\n",
    "                                                     6: 2,\n",
    "                                                     7: 1,\n",
    "                                                     8: 2,\n",
    "                                                     9: 2,\n",
    "                                                     10: 1,\n",
    "                                                     11: 2,\n",
    "                                                     12: 2}, inplace = True)\n",
    "        x['PromoInterval_Mar,Jun,Sept,Dec'].replace({1: 2,\n",
    "                                                     2: 2,\n",
    "                                                     3: 1,\n",
    "                                                     4: 2,\n",
    "                                                     5: 2,\n",
    "                                                     6: 1,\n",
    "                                                     7: 2,\n",
    "                                                     8: 2,\n",
    "                                                     9: 1,\n",
    "                                                     10: 2,\n",
    "                                                     11: 2,\n",
    "                                                     12: 1}, inplace = True)\n",
    "        x.loc[(x.year<x.Promo2SinceYear) | ((x.year==x.Promo2SinceYear) & (x.weekofyear<x.Promo2SinceWeek)),\n",
    "             ['PromoInterval_Mar,Jun,Sept,Dec', 'PromoInterval_Jan,Apr,Jul,Oct', 'PromoInterval_Feb,May,Aug,Nov']] = 0\n",
    "        x.loc[:, 'Promo2_status'] = x[['PromoInterval_Mar,Jun,Sept,Dec', 'PromoInterval_Jan,Apr,Jul,Oct', 'PromoInterval_Feb,May,Aug,Nov']].max(axis = 1)\n",
    "        return x\n",
    "    merge_list = list()\n",
    "    merge_list.append(train[(train.Open == 1) & (train.Sales > 0)].groupby('Store').Sales.mean().to_frame().reset_index().rename(columns = {'Sales': 'mean_Sales'}))\n",
    "#     merge_list.append(train[(train.Open == 1) & (train.Sales > 0)].groupby('Store').Sales.median().to_frame().reset_index().rename(columns = {'Sales': 'median_Sales'}))\n",
    "    merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').Customers.mean().to_frame().reset_index().rename(columns = {'Customers': 'mean_Customers'}))\n",
    "#     merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').Customers.median().to_frame().reset_index().rename(columns = {'Customers': 'median_Customers'}))\n",
    "    merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').apply(frac_mean).to_frame().reset_index().rename(columns = {0: 'mean_frac'}))\n",
    "    merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').apply(kurt).reset_index())\n",
    "    merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').apply(sk).reset_index())\n",
    "#     merge_list.append(train[(train.Open == 1) & (train.Customers > 0)].groupby('Store').apply(frac_median).to_frame().reset_index().rename(columns = {0: 'median_frac'}))\n",
    "    def create_store_feat(data):\n",
    "        data.loc[:,'day_from_beg'] = (data.year - 2013)*365 + data.dayofyear\n",
    "        store_feat = data.groupby('Store').\\\n",
    "            apply(lambda x: pd.Series([np.corrcoef(x.day_from_beg, x.Sales)[0,1]*x.Sales.std()/x.day_from_beg.std(),\n",
    "                                                         x.Sales.std()], index=['trend', 'sales_std']))\n",
    "        return store_feat.reset_index()\n",
    "    merge_list.append(create_store_feat(train[(train.Open == 1) & (train.Sales > 100)]))\n",
    "    \n",
    "    train = preprocess_fixed_effects(train, merge_list)\n",
    "    test = preprocess_fixed_effects(test, merge_list)\n",
    "    \n",
    "    month_mean_sales = train.groupby(['Store', 'month'])['Sales'].mean().reset_index().rename(columns = {'Sales': 'mean_month_sales'})\n",
    "    train = pd.merge(train, month_mean_sales, on = ['Store', 'month'])\n",
    "    test = pd.merge(test, month_mean_sales, on = ['Store', 'month'])\n",
    "    # Добавляю таблицу Store \n",
    "    store['CompetitionDistance'].fillna(75860., inplace = True)\n",
    "    train = train.merge(store[['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'Promo2', 'PromoInterval', 'CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth', 'Promo2SinceYear', 'Promo2SinceWeek']], on = 'Store', how = 'left')\n",
    "    test = test.merge(store[['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'Promo2', 'PromoInterval', 'CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth', 'Promo2SinceYear', 'Promo2SinceWeek']], on = 'Store', how = 'left')\n",
    "    # делаем дамми. \n",
    "    train.StateHoliday.replace(0, '0', inplace = True)\n",
    "    test.StateHoliday.replace(0, '0', inplace = True)\n",
    "    le = LabelEncoder()\n",
    "    for i in ['StateHoliday', 'StoreType', 'Assortment']:\n",
    "        le.fit(train[i].append(test[i]))\n",
    "        train.loc[:, i] = le.transform(train[i])\n",
    "        test.loc[:, i] = le.transform(test[i])\n",
    "    train = pd.get_dummies(train, columns = ['PromoInterval'])\n",
    "    test = pd.get_dummies(test, columns = ['PromoInterval'])\n",
    "\n",
    "\n",
    "    train['competitor_time_distance'] = ((train['year'] - train['CompetitionOpenSinceYear'])*12 + (train['month'] - train['CompetitionOpenSinceMonth']))\n",
    "    train['competitor_exists'] = (train['competitor_time_distance']>0).astype(int)\n",
    "    train['competitor_time_distance'] = train['competitor_time_distance'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    test['competitor_time_distance'] = ((test['year'] - test['CompetitionOpenSinceYear'])*12 + (test['month'] - test['CompetitionOpenSinceMonth']))\n",
    "    test['competitor_exists'] = (test['competitor_time_distance']>0).astype(int)\n",
    "    test['competitor_time_distance'] = test['competitor_time_distance'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Часть выверки одинаковости колонок в тесте и трейне\n",
    "    train_cols = list(train.columns)\n",
    "    test_cols = list(test.columns)\n",
    "    if 'Sales' in train_cols:\n",
    "        train_cols.remove('Sales')\n",
    "    if 'Sales' in test_cols:\n",
    "        test_cols.remove('Sales')\n",
    "    if 'Customers' in train_cols:\n",
    "        train_cols.remove('Customers')\n",
    "#         del train['Customers']\n",
    "    test_cols = list(test_cols)\n",
    "    if 'Customers' in test_cols:\n",
    "        test_cols.remove('Customers')\n",
    "#         del test['Customers']\n",
    "    if 'Id' in test_cols:\n",
    "        test_cols.remove('Id')\n",
    "    # На всякий случай выверяем, что колонки в тесте и трейне одинаковы\n",
    "    if sorted(train_cols) != sorted(test_cols):\n",
    "        for i in set(train_cols).difference(set(test_cols)):\n",
    "            test.loc[:, i] = 0\n",
    "        for i in set(test_cols).difference(set(train_cols)):\n",
    "            train.loc[:, i] = 0\n",
    "    train = replace_promo(train)\n",
    "    test = replace_promo(test)\n",
    "#     train.drop(labels = ['Store', 'Date'], axis = 1, inplace = True)\n",
    "#     test.drop(labels = ['Store', 'Date'], axis = 1, inplace = True)\n",
    "    train.drop(labels = ['Date'], axis = 1, inplace = True)\n",
    "    test.drop(labels = ['Date'], axis = 1, inplace = True)\n",
    "    \n",
    "    all_cols = list(set(train_cols).union(set(test_cols)))\n",
    "    all_cols.append('Promo2_status')\n",
    "#     all_cols.remove('Store')\n",
    "    all_cols.remove('Date')\n",
    "    all_cols.remove('Open')\n",
    "    all_cols.remove('CompetitionOpenSinceYear')\n",
    "    all_cols.remove('CompetitionOpenSinceMonth')\n",
    "    test.fillna(1, inplace = True)\n",
    "    train.loc[:, 'till_holiday'] = train.loc[:, 'till_holiday'].apply(lambda x: min(x, 14))\n",
    "    train.loc[:, 'from_holiday'] = train.loc[:, 'from_holiday'].apply(lambda x: min(x, 14))\n",
    "    test.loc[:, 'till_holiday'] = test.loc[:, 'till_holiday'].apply(lambda x: min(x, 14))\n",
    "    test.loc[:, 'from_holiday'] = test.loc[:, 'from_holiday'].apply(lambda x: min(x, 14))\n",
    "    if scaler:\n",
    "        train.fillna(0, inplace = True)\n",
    "        test.fillna(1, inplace = True)\n",
    "        scaler.fit(train.loc[train.Open == 1, all_cols].append(test.loc[test.Open == 1, all_cols], ignore_index = True))\n",
    "        train.loc[:, all_cols] = scaler.transform(train[all_cols])\n",
    "        test.loc[:, all_cols] = scaler.transform(test[all_cols])\n",
    "        \n",
    "    train = train[train.Open == 1]\n",
    "    train = train[train.Sales > 0]\n",
    "    \n",
    "    return train, test, all_cols\n",
    "\n",
    "def make_train_test(x, size = 45, same_period = False):\n",
    "    if same_period:\n",
    "        return x[(x.Date < pd.to_datetime('2013-08-01')) | (x.Date > pd.to_datetime('2013-09-17'))], x[(x.Date >=pd.to_datetime('2013-08-01')) & (x.Date <= pd.to_datetime('2013-09-17'))]\n",
    "    else:\n",
    "        max_date = x.Date.max()\n",
    "        return x[x.Date < (max_date - np.timedelta64(size, 'D'))], x[x.Date >= (max_date - np.timedelta64(size, 'D'))] \n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/Anaconda3/lib/python3.4/site-packages/pandas/core/indexing.py:266: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/conda/Anaconda3/lib/python3.4/site-packages/pandas/core/indexing.py:426: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "validation = True\n",
    "test_stores = te.Store.unique()\n",
    "if validation:\n",
    "    tr.Date = pd.to_datetime(tr.Date, format = '%Y-%m-%d')\n",
    "    tr, te = make_train_test(tr, 47, same_period = False)\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, features = preprocess_train_data(tr, te, store)\n",
    "# features.remove('Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/Anaconda3/lib/python3.4/site-packages/ipykernel/__main__.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "states = pd.read_csv('external/store_states.csv')\n",
    "weather = pd.read_pickle('external/weather_ready')\n",
    "\n",
    "use_weather_cols = [name for name in weather.columns if not ('Max_' in name or 'Min_' in name)]\n",
    "new_weather_cols = use_weather_cols.copy()\n",
    "for col in use_weather_cols:\n",
    "    if any(weather[col].isnull()):\n",
    "        weather[col+'_nan'] = weather[col].isnull().astype(np.int8)\n",
    "        new_weather_cols.append(col+'_nan')\n",
    "\n",
    "weather = weather[new_weather_cols]\n",
    "weather.fillna(weather.mean(), inplace=True)\n",
    "\n",
    "weather['Date'] = pd.to_datetime(weather.Date, format='%Y-%m-%d')\n",
    "\n",
    "weather['day'] = weather.Date.dt.day\n",
    "weather['month'] = weather.Date.dt.month\n",
    "weather['year'] = weather.Date.dt.year\n",
    "\n",
    "relevant = ['State', 'Events', 'Mean_TemperatureC', 'Mean_Sea_Level_PressurehPa', 'Precipitationmm', 'year', 'month', 'day']\n",
    "weather = weather[relevant]\n",
    "\n",
    "\n",
    "\n",
    "X_train = X_train.merge(states, on='Store', how='inner')\n",
    "X_test = X_test.merge(states, on='Store', how='inner')\n",
    "\n",
    "X_train = X_train.merge(weather, on=['State', 'year', 'month', 'day'])\n",
    "X_test = X_test.merge(weather, on=['State', 'year', 'month', 'day'])\n",
    "\n",
    "X_train.Events.fillna('nan', inplace=True)\n",
    "X_test.Events.fillna('nan', inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(np.hstack([X_train.State.values, X_test.State.values]))\n",
    "X_train.State = le.transform(X_train.State.values)\n",
    "X_test.State = le.transform(X_test.State.values)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.hstack([X_train.Events.values, X_test.Events.values]))\n",
    "X_train.Events = le.transform(X_train.Events.values)\n",
    "X_test.Events = le.transform(X_test.Events.values)\n",
    "\n",
    "\n",
    "championship = pd.DataFrame([[1,16,6,2014],\n",
    "                            [1,21,6,2014],\n",
    "                            [1,26,6,2014],\n",
    "                            [1,30,6,2014],\n",
    "                            [1,4,7,2014],\n",
    "                            [1,8,7,2014],\n",
    "                            [1,13,7,2014]], columns = ['is_match', 'day', 'month', 'year'])\n",
    "\n",
    "def add_champ(X_train):\n",
    "    X_train = X_train.merge(championship, on =  ['day', 'month', 'year'], how = 'left').fillna(0)\n",
    "\n",
    "    X_train['is_champ'] = 0\n",
    "    X_train['is_champ'][(X_train.year == 2014) & \n",
    "                        (((X_train.month == 6) & (X_train.day >= 16)) | ((X_train.month == 7) & (X_train.day <= 13)))] = 1\n",
    "    return X_train\n",
    "\n",
    "X_train = add_champ(X_train)\n",
    "X_test = add_champ(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train[(X_train.till_holiday>1) & (X_train.from_holiday>1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion of some features. It is easier to make here instead of changing preprocessing function (it works fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.append('skew')\n",
    "features.append('kurt')\n",
    "features.remove('sk')\n",
    "features.remove('ku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l =( ['till_closed', \n",
    "     'from_promo', 'from_holiday',\n",
    "     'from_school_holiday', 'till_promo', 'till_school', 'till_holiday', \n",
    "    'from_closed'] \n",
    "   +  ['PromoInterval_Feb,May,Aug,Nov', 'PromoInterval_Mar,Jun,Sept,Dec', 'PromoInterval_Jan,Apr,Jul,Oct']  \n",
    "        +              ['Promo2SinceWeek', 'Promo2SinceYear', 'Events', 'StateHoliday',\n",
    "                       'Precipitationmm', 'is_match', 'is_champ', 'mean_month_sales']\n",
    "#   + ['Promo2']\n",
    "   )\n",
    "for i in l:\n",
    "    if i in features:\n",
    "        features.remove(i)\n",
    "# features.append('from_closed')\n",
    "# features.append('from_promo')\n",
    "# features.append('from_school_holiday')\n",
    "# features.append('from_holiday')\n",
    "# features.append('till_holiday')\n",
    "\n",
    "features.append('State')\n",
    "# features.append('Events')\n",
    "features.append('Mean_TemperatureC')\n",
    "features.append('Mean_Sea_Level_PressurehPa')\n",
    "features.append('Precipitationmm')\n",
    "# features.append('is_match')\n",
    "# features.append('is_champ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete closed stores from validation, this is how I validate\n",
    "X_test = X_test[X_test.Open == 1]\n",
    "X_test.reset_index(drop = True, inplace = True)\n",
    "ind = list(X_test[X_test.Store.isin(test_stores)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35262, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[ind].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747919, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt in outlier deletion. Didn't succeed, therefore never launch this cell\n",
    "Простой способ определить выброс - пробежаться скользящим средним по значениям в Sales. Я ожидаю, что количество выбросов не превышает процента, подгоню параметр скользящего среднего под это значение. \n",
    "Формула примерно такая: $x_{ i, t}=\\min\\left(Rolling\\,window_{ i, t},\\, x_{ i, t}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего найдено выбросов в 1115 магазинах: 421.\n",
      "Всего наблюдений 747919, относительное число выбросов 0.0005628951798256228\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "coeff = 3.5\n",
    "num_outliers = 0\n",
    "num_shops = X_train.Store.nunique()\n",
    "rolling_window = 10\n",
    "num_observations = 0\n",
    "for i in X_train.Store.unique():\n",
    "    temp_series = pd.rolling_mean(X_train[X_train.Store == i].Sales, rolling_window)\n",
    "    ind = (X_train[X_train.Store == i].Sales > temp_series*coeff) | (X_train[X_train.Store == i].Sales*coeff < temp_series)\n",
    "    num_outliers += ind.sum()\n",
    "    temp_series_2 = pd.rolling_mean(X_train[X_train.Store == i].Sales, 2)\n",
    "    X_train.loc[(X_train.Store == i) & ind, 'Sales'] = temp_series_2.loc[ind]\n",
    "#     X_train.iloc[list(ind.index)].loc[ind, 'Sales'] = temp_series_2.loc[ind]\n",
    "    num_observations += X_train[X_train.Store == i].shape[0]\n",
    "print('Всего найдено выбросов в {0} магазинах: {1}.\\nВсего наблюдений {2}, относительное число выбросов {3}'\n",
    "      .format(num_shops, num_outliers, num_observations, num_outliers/num_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train[features], X_train[\"Sales\"] )\n",
    "if validation:\n",
    "    dvalid = xgb.DMatrix(X_test[features], X_test[\"Sales\"])\n",
    "else:\n",
    "    dtest = xgb.DMatrix(X_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45884, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test:  0.107043538022 , Train:  0.0815677329763 time -  0:09:13.488695\n",
      "1 Test:  0.106052421883 , Train:  0.0792847638172 time -  0:18:19.902588\n",
      "2 Test:  0.107081787974 , Train:  0.0816014501675 time -  0:27:15.563731\n",
      "3 Test:  0.107587830181 , Train:  0.0792083075691 time -  0:36:19.704748\n",
      "4 Test:  0.108992415644 , Train:  0.0819524176216 time -  0:45:22.640674\n",
      "average  0.105843098476 , time -  0:45:22.688845\n",
      "geometric average  0.105840778341 , time -  0:45:22.688928\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import datetime\n",
    "r = datetime.datetime.now()\n",
    "dtrain = xgb.DMatrix(X_train[features], np.log(X_train[\"Sales\"] + 1))\n",
    "# dtrain = xgb.DMatrix(X_train[features], X_train[\"Sales\"])\n",
    "if validation:\n",
    "    dvalid = xgb.DMatrix(X_test[features], np.log(X_test[\"Sales\"] + 1))\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "else:\n",
    "    dtest = xgb.DMatrix(X_test[features])\n",
    "\n",
    "e = True\n",
    "d = 5\n",
    "for i in range(d):\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "          \"learning_rate\": 0.061,\n",
    "          \"max_depth\": 11,\n",
    "          \"subsample\": 0.8,\n",
    "          \"colsample_bytree\":0.7,\n",
    "          \"silent\": 1,\n",
    "          'min_child_weight': 11,\n",
    "        'seed':i + 22,\n",
    "        'nthread':4}\n",
    "\n",
    "    num_trees = 700\n",
    "    if not validation:\n",
    "        gbm = xgb.train(params, \n",
    "                        dtrain, \n",
    "                        num_trees, \n",
    "#                         evals=watchlist, \n",
    "#                         early_stopping_rounds=50, \n",
    "#                         feval=rmspe_xg, \n",
    "    #                     verbose_eval=True\n",
    "                       )\n",
    "    else:\n",
    "        gbm = xgb.train(params, \n",
    "                    dtrain, \n",
    "                    num_trees, \n",
    "#                         evals=watchlist, \n",
    "#                         early_stopping_rounds=50, \n",
    "#                         feval=rmspe_xg, \n",
    "#                         verbose_eval=False\n",
    "                   )\n",
    "\n",
    "    train_probs = gbm.predict(dtrain)\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error_train = rmspe(np.exp(train_probs) - 1, X_train['Sales'].values)\n",
    "#     error_train = rmspe(train_probs, X_train['Sales'].values)\n",
    "    if validation:\n",
    "        test_probs = gbm.predict(dvalid)\n",
    "    else:\n",
    "        test_probs = gbm.predict(dtest)\n",
    "    indices = test_probs < 0.\n",
    "    test_probs[indices] = 0\n",
    "    if validation:\n",
    "        error = rmspe(np.exp(test_probs[ind]) - 1, X_test.iloc[ind]['Sales'].values)\n",
    "        print(i, 'Test: ', error,', Train: ', error_train , 'time - ', datetime.datetime.now()- r)\n",
    "    else:\n",
    "        print(i, ', Train: ', error_train, ', time - ', datetime.datetime.now()- r)\n",
    "    if type(e) == bool:\n",
    "        e_geom = np.log(test_probs.copy())/d\n",
    "        e = test_probs.copy()/d\n",
    "    else:\n",
    "        e += test_probs/d\n",
    "        e_geom += np.log(test_probs)/d\n",
    "if validation:\n",
    "    error_geom = rmspe(np.exp(np.exp(e_geom[ind]))-1, X_test.iloc[ind]['Sales'].values)\n",
    "    error = rmspe(np.exp(e[ind])-1, X_test.iloc[ind]['Sales'].values)\n",
    "    print('average ', error, ', time - ', datetime.datetime.now()- r)\n",
    "    print('geometric average ', error_geom, ', time - ', datetime.datetime.now()- r)\n",
    "else:\n",
    "    submission = pd.DataFrame({\"Id\": X_test[\"Id\"], \"Sales\": np.exp(e) - 1})\n",
    "    submission_geom = pd.DataFrame({\"Id\": X_test[\"Id\"], \"Sales\": np.exp(np.exp(e_geom)) - 1})\n",
    "#     submission = pd.DataFrame({\"Id\": X_test[\"Id\"], \"Sales\": e})\n",
    "    submission.to_csv(\"700_moments_no_football.csv\", index=False)\n",
    "    submission_geom.to_csv(\"700_moments_no_football_geom.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features=0.6, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=500, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, \n",
    "                           criterion='mse', \n",
    "                           max_depth=5, \n",
    "                           min_samples_split=2, \n",
    "                           min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.0, \n",
    "                           max_features=0.6, \n",
    "                           max_leaf_nodes=None, \n",
    "                           bootstrap=True, \n",
    "                           oob_score=False, \n",
    "                           n_jobs=-1, \n",
    "                           random_state=None, \n",
    "                           verbose=0, \n",
    "                           warm_start=False)\n",
    "rf.fit(X_train[features], np.log(X_train['Sales']+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(len(features)):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]), features[indices[f]])\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(len(features)), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(len(features)), indices)\n",
    "plt.xlim([-1, len(features)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt.\n",
    "I average several predictions for the validation set, as this greatly improves the prediction quality of the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=50,\n",
    "#                                 max_features = space['max_features'], \n",
    "#                                  max_depth = space['max_depth'],\n",
    "#                                  min_samples_leaf = space['min_samples_leaf'],\n",
    "#                                 n_jobs = -1)\n",
    "#################\n",
    "    loss = 0.\n",
    "    pred = True\n",
    "    for i in range(4):\n",
    "        params = {\"objective\": \"reg:linear\",\n",
    "                  \"learning_rate\": space['learning_rate'],\n",
    "                  \"max_depth\": space['max_depth'],\n",
    "                  \"subsample\": space['subsample'],\n",
    "                  \"colsample_bytree\": space['colsample_bytree'],\n",
    "                  \"silent\": 1,\n",
    "                  'min_child_weight': space['min_child_weight'],\n",
    "            'seed':i*100+22,\n",
    "            'nthread':4\n",
    "                  }\n",
    "        \n",
    "#         clf = xgb.XGBRegressor(n_estimators = 1000, missing = -1., \n",
    "#                                 max_depth = space['max_depth'],\n",
    "#                                 min_child_weight = space['min_child_weight'],\n",
    "#                                 subsample = space['subsample'],\n",
    "#                                 colsample_bytree = space['colsample_bytree'],\n",
    "#                                 learning_rate = space['learning_rate'],\n",
    "#                                nthread = 2,\n",
    "#                                objective = 'reg:linear')\n",
    "        num_trees = 1000\n",
    "        gbm = xgb.train(params, \n",
    "                        dtrain, \n",
    "                        num_trees, \n",
    "#                         evals=watchlist, \n",
    "#                         early_stopping_rounds=50, \n",
    "                        feval=rmspe_xg, \n",
    "#                         verbose_eval=True\n",
    "                       )\n",
    "        test_probs = gbm.predict(dvalid)\n",
    "        indices = test_probs < 0\n",
    "        test_probs[indices] = 0\n",
    "        min_error = rmspe(test_probs, X_test['Sales'].values)\n",
    "        train_probs = gbm.predict(dtrain)\n",
    "        indices = train_probs < 0\n",
    "        train_probs[indices] = 0\n",
    "        error_train = rmspe(train_probs, X_train['Sales'].values)\n",
    "        if type(pred) == bool:\n",
    "            pred = test_probs.copy()/4.\n",
    "        else:\n",
    "            pred += test_probs/4.\n",
    "        print(min_error, \"train_error:\", error_train,\n",
    "              space['learning_rate'], space['max_depth'], \n",
    "              space['min_child_weight'], space['subsample'],\n",
    "              space['colsample_bytree'], i)\n",
    "        if min_error>108:\n",
    "            print('Big error: {0}'.format(min_error))\n",
    "            min_error = 0.3\n",
    "            break\n",
    "    if min_error != 0.3:\n",
    "        min_error = rmspe(pred, X_test['Sales'].values)\n",
    "#         error = rmspe(np.exp(pred/4.) - 1, X_test['Sales'].values)\n",
    "        print('Obtained error {0}'.format(min_error))\n",
    "        with open('no_logs.txt', 'a') as f:\n",
    "            f.write(str(min_error) + ' ' + str(space['learning_rate']) + ' ' + str(space['max_depth']) + ' ' +\n",
    "                   str(space['min_child_weight']) + ' ' + str(space['subsample']) + ' ' + str(space['colsample_bytree']))\n",
    "    else:\n",
    "        print('error too big')\n",
    "    return{'loss':min_error, 'status': STATUS_OK } \n",
    "\n",
    "###### RF parameter space #######\n",
    "# space ={\n",
    "#         'max_depth': hp.quniform(\"x_max_depth\", 5, 30, 1),\n",
    "#         'min_samples_leaf': hp.quniform ('min_samples_leaf', 1, 10, 1),\n",
    "#         'max_features': hp.uniform ('max_features', 0.5, 0.9)\n",
    "#     }\n",
    "\n",
    "###### XGB parameter space #######\n",
    "space ={\n",
    "        'max_depth': hp.quniform(\"max_depth\", 8, 16, 1),\n",
    "        'min_child_weight': hp.quniform ('min_child_weight', 3, 15, 2),\n",
    "        'subsample': hp.uniform ('subsample', 0.4, 0.9),\n",
    "            'colsample_bytree': hp.uniform ('colsample_bytree', 0.4, 0.9),\n",
    "    'learning_rate': hp.uniform('lr', 0.02, 0.08)\n",
    "    }\n",
    "\n",
    "###### LinearSVC parameter space #######\n",
    "# space ={\n",
    "#         'C': hp.uniform ('C', 0.001, 5),\n",
    "#         'intercept_scaling': hp.uniform ('intercept_scaling', 0.001, 10),\n",
    "#         'dual': hp.choice('dual', [True, False])\n",
    "# #         'penalty': hp.choice('penalty', ['l1', 'l2'])\n",
    "   \n",
    "#     }\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=75,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
